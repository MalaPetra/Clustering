---
title: "Cluster Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Course Description

Cluster analysis is a powerful toolkit in the data science workbench. It is used to find groups of observations (clusters) that share similar characteristics. These similarities can inform all kinds of business decisions; for example, in marketing, it is used to identify distinct groups of customers for which advertisements can be tailored. In this course, you will learn about two commonly used clustering methods - hierarchical clustering and k-means clustering. You won't just learn how to use these methods, you'll build a strong intuition for how they work and how to interpret their results. You'll develop this intuition by exploring three different datasets: soccer player positions, wholesale customer spending data, and longitudinal occupational wage data.


1. Calculating distance between observations
2. Hierarchical clustering
3. K-means clustering
4. Case Study: National Occupational mean wage

```{r message=FALSE, warning=FALSE}
# This sets plot images to a nice size.
options(repr.plot.width = 4, repr.plot.height = 4)

# Loading in the ggplot2 package
library(dplyr)
library(ggplot2)
```

```{r message=FALSE, warning=FALSE}
#install.packages('dummies', repos='http://cran.us.r-project.org')
#install.packages('dendextend', repos='http://cran.us.r-project.org')

library(dummies)
library(dendextend)
library(readxl)
library(purrr)
library(cluster)
```

# 1. Calculating distance between observations

Cluster analysis seeks to find groups of observations that are similar to one another, but the identified groups are different from each other. This similarity/difference is captured by the metric called distance. In this chapter, you will learn how to calculate the distance between observations for both continuous and categorical features. You will also develop an intuition for how the scales of your features can affect distance.

### Calculate & plot the distance between two players

You've obtained the coordinates relative to the center of the field for two players in a soccer match and would like to calculate the distance between them.
In this exercise you will plot the positions of the 2 players and manually calculate the distance between them by using the euclidean distance formula

**INSTRUCTIONS**

- Plot their positions from the two_players dataframe    using ggplot
- Extract the positions of the players into two data frames player1 and player2
- Calculate the distance between player1 and player2 by using the euclidean distance formula
$$\sqrt{(x_1−x_2)^2+(y_1−y_2)^2}$$

```{r}
two_players = data.frame(x=c(5, 15),y=c(4, 10))
two_players
```

```{r}
# Plot the positions of the players
ggplot(two_players, aes(x = x, y = y)) + 
  geom_point() +
  # Assuming a 40x60 field
  lims(x = c(-30,30), y = c(-20, 20))

# Split the players data frame into two observations
player1 <- two_players[1,]
player2 <- two_players[2,]

# Calculate and print their distance using the Euclidean Distance formula
player_distance <- sqrt( (player1$x - player2$x)^2 + (player1$y - player2$y)^2 )
player_distance
```

### Using the dist() function

Using the euclidean formula manually may be practical for 2 observations but can get more complicated rather quickly when measuring the distance between many observations.
The dist() function simplifies this process by calculating distances between our observations (rows) using their features (columns). In this case the observations are the player positions and the dimensions are their x and y coordinates.

Note: The default distance calculation for the dist() function is euclidean distance

**INSTRUCTIONS**

- Calculate the distance between two players using the dist() function for the dataframe two_players
- Calculate the distance between three players for the dataframe three_players

```{r}
three_players = data.frame(x = c(5, 15, 0), y = c(4, 10, 20))
```

```{r}
# Calculate the Distance Between two_players
dist_two_players <- dist(two_players)
dist_two_players

# Calculate the Distance Between three_players
dist_three_players <- dist(three_players)
dist_three_players
```

### The importance of scale

You have learned that when a variable is on a larger scale than other variables in your data it may disproportionately influence the resulting distance calculated between your observations. Lets see this in action by observing a sample of data from the trees data set.

You will leverage the scale() function which by default centers & scales our column features.

Our variables are the following:

Girth - tree diameter in inches
Height - tree height in inches

**INSTRUCTIONS**

- Calculate the distance matrix for the dataframe three_trees and store it as dist_trees
- Create a new variable scaled_three_trees where the three_trees data is centered & scaled
- Calculate and print the distance matrix for scaled_three_trees and store this as dist_scaled_trees
- Output both dist_trees and dist_scaled_trees matrices and observe the change of which observations have the smallest distance between the two matrices (hint: they have changed)

```{r}
three_trees = data.frame(Girth = c(8.3, 8.6, 10.5), Height = c(840, 780, 864))
three_trees
```

```{r}
# Calculate distance for three_trees 
dist_trees <- dist(three_trees)

# Scale three trees & calculate the distance  
scaled_three_trees <- scale(three_trees)
dist_scaled_trees <- dist(scaled_three_trees)

# Output the results of both Matrices
print('Without Scaling')
dist_trees

print('With Scaling')
dist_scaled_trees
```

Notice that before scaling observations 1 & 3 were the closest but after scaling observations 1 & 2 turn out to have the smallest distance.

### Measuring distance for categorical data

Calculating distance between categorical variables In this exercise you will explore how to calculate binary (Jaccard) distances. In order to calculate distances will first have to dummify our categories using the dummy.data.frame() from the library dummies

You will use a small collection of survey observations stored in the data frame job_survey with the following columns:

- job_satisfaction Possible options: "Hi", "Mid", "Low"
- is_happy Possible options: "Yes", "No"

**INSTRUCTIONS**

- Create a dummified dataframe dummy_survey
- Generate a Jaccard distance matrix for the dummified survey data dist_survey using the dist() function using the parameter method = 'binary'
- Print the original data and the distance matrix
- Note the observations with a distance of 0 in the original data (1, 2, and 4)

```{r}
job_satisfaction = c("Low", "Low", "Hi", "Low", "Mid")
is_happy = c("No", "No", "Yes", "No", "No")
job_survey = data.frame(job_satisfaction=job_satisfaction,is_happy=is_happy)
```

```{r}
job_survey
```

```{r}
# library(dummies)
```

```{r}
# Dummify the Survey Data
dummy_survey <- dummy.data.frame(job_survey)

# Calculate the Distance
dist_survey <- dist(dummy_survey, method="binary")

# Print the Original Data
job_survey

# Print the Distance Matrix
dist_survey
```

Notice that this distance metric successfully captured that observations 1 and 2 are identical (distance of 0)

# 2. Hierarchical clustering

This chapter will help you answer of how do you find groups of similar observations (clusters) in your data using the distances. You will learn about the fundamental principles of hierarchical clustering - the linkage criteria and the dendrogram plot - and how both are used to build clusters. You will also explore data from a wholesale distributor in order to perform market segmentation of clients using their spending habits.

### Comparing more than two observations, calculating linkage

Let us revisit the example with three players on a field. The distance matrix between these three players is shown below and is available as the variable dist_players.

From this we can tell that the first group that forms is between players 1 & 2, since they are the are the closest to one another with a euclidean distance value of 11.

Now you want to apply the three linkage methods you have learned to determine what the distance of this group is to player 3.


**INSTRUCTIONS**

- Extract the distance values between all three pairs of players into individual variables
- Calculate the distance from player 3 to the group of players 1 & 2 using the following three linkage methods:

  - Complete: the resulting distance is based on the   maximum
  - Single: the resulting distance is based on the minimum
  - Average: the resulting distance is based on the average

```{r}
dist_players = structure(c(11.6619037896906, 16.7630546142402, 18.0277563773199
), Size = 3L, Diag = FALSE, Upper = FALSE, method = "euclidean", call = dist(x = three_players), class = "dist")
```

```{r}
# Extract the pair distances
distance_1_2 <- dist_players[1]
distance_1_3 <- dist_players[2]
distance_2_3 <- dist_players[3]

# Calculate the complete distance between group 1-2 and 3
max(c(distance_1_3, distance_2_3))

# Calculate the single distance between group 1-2 and 3
min(c(distance_1_3, distance_2_3))

# Calculate the average distance between group 1-2 and 3
mean(c(distance_1_3, distance_2_3))
```

The closest observation to a pair?

```{r}
four_players = structure(list(x = c(5, 15, 0, -5), y = c(4, 10, 20, 5)), .Names = c("x", "y"), row.names = c(NA, -4L), class = "data.frame")
```

```{r}
dist_players = dist(four_players)
```

```{r}
plot(hclust(dist_players, "complete"))
```

```{r}
plot(hclust(dist_players, "single"))
```

```{r}
plot(hclust(dist_players, "average"))
```

Complete Linkage: Player 3, Single & Average Linkage: Player 2

### Capturing K clusters

In this exercise you will leverage the hclust() function to calculate the iterative linkage steps and you will use the cutree() function to extract the cluster assignments for the desired number (k) of clusters.

You are given the positions of 12 players at the start of a 6v6 soccer match. This is stored in the lineup dataframe.

You know that this match has two teams (k = 2), let's use the clustering methods your learned to assign which team each player belongs in based on their position.

Notes:

- The linkage method can be passed via the method parameter: hclust(distance_matrix, method = "complete")
- Remember that in soccer opposing teams start on their half of the field.
- Because these positions are measured using the same scale we do not need to re-scale our data.

**INSTRUCTIONS**

- Calculate the euclidean distance matrix dist_players among all twelve players
- Perform the complete linkage calculation for hierarchical clustring using hclust and store this as hc_players
- Build the cluster assignment vector clusters_k2 using cutree() with a k = 2
- Append the cluster assignments as a column cluster to the lineup data frame and save the results to a new dataframe called lineup_k2_complete

```{r}
lineup = structure(list(x = c(-1, -2, 8, 7, -12, -15, -13, 15, 21, 12, 
-25, 26), y = c(1, -3, 6, -8, 8, 0, -10, 16, 2, -15, 1, 0)), .Names = c("x", 
"y"), class = c("tbl_df", "tbl", "data.frame"), row.names = c(NA, 
-12L))
```

```{r}
# Calculate the Distance
dist_players <- dist(lineup)

# Perform the hierarchical clustering using the complete linkage
hc_players <- hclust(dist_players, method="complete")

# Calculate the assignment vector with a k of 2
clusters_k2 <- cutree(hc_players, k=2)

# Create a new dataframe storing these results
lineup_k2_complete <- mutate(lineup, cluster = clusters_k2)
```

### Exploring the clusters

Because clustering analysis is always in part qualitative, it is incredibly important to have the necessary tools to explore the results of the clustering.

In this exercise you will explore that data frame you created in the previous exercise lineup_k2_complete.

Reminder: The lineup_k2_complete dataframe contains the x & y positions of 12 players at the start of a 6v6 soccer game to which you have added clustering assignments based on the following parameters:

Distance: Euclidean Number of Clusters (k): 2 
Linkage Method: Complete 

**INSTRUCTIONS**

- Using count() from dplyr, count the number of players assigned to each cluster.
- Using ggplot(), plot the positions of the players and color them by cluster assignment.

```{r}
# Count the cluster assignments
count(lineup_k2_complete, cluster)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_k2_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

### Visualizing the Dendogram, Comparing average, single and complete linkage

You are now ready to analyze the clustering results of the lineup dataset using the dendrogram plot. This will give you a new perspective on the effect the decision of the linkage method has on your resulting cluster analysis.

**INSTRUCTIONS**

- Perform the linkage calculation for hierarchical clustring using the linkages: complete, single and average
- Plot the three dendrograms side by side and review the changes

```{r}
# Prepare the Distance Matrix
dist_players <- dist(lineup)

# Generate hclust for complete, single & average linkage methods
hc_complete <- hclust(dist_players, method="complete")
hc_single <- hclust(dist_players, method="single")
hc_average <- hclust(dist_players, method="average")

# Plot & Label the 3 Dendrograms Side-by-Side
# Hint: To see these Side-by-Side run the 4 lines together as one command
par(mfrow = c(1,3))
plot(hc_complete, main = 'Complete Linkage')
plot(hc_single, main = 'Single Linkage')
plot(hc_average, main = 'Average Linkage')
```

### Cutting the tree, clusters based on height

In previous exercises you have grouped your observations into clusters using a pre-defined number of clusters (k). In this exercise you will leverage the visual representation of the dendrogram in order to group your observations into clusters using a maximum height (h), below which clusters form.

You will work the color_branches() function from the dendextend library in order to visually inspect the clusters that form at any height along the dendrogram.

The hc_players has been carried over from your previous work with the soccer line-up data.

**INSTRUCTIONS**

- Create a denrogram object dend_players from your hclust result using the function as.dendrogram()
- Plot the dendrogram
- Using the color_branches() function create & plot a new dendrogram with clusters colored by a cut height of 20
- Repeat the above step with a height of 40

```{r}
# library(dendextend)
```

```{r}
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = "complete")

# Create a dendrogram object from the hclust variable
dend_players <- as.dendrogram(hc_players)

# Plot the dendrogram
plot(dend_players)

# Color brances by cluster formed from the cut at a height of 20 & plot
dend_20 <- color_branches(dend_players, h = 20)

# Plot the dendrogram with clusters colored below height 20
plot(dend_20)

# Color brances by cluster formed from the cut at a height of 40 & plot
dend_40 <- color_branches(dend_players, h = 40)

# Plot the dendrogram with clusters colored below height 40
plot(dend_40)
```

Can you see that the height that you use to cut the tree greatly influences the number of clusters and their size? Consider taking a moment to play with other values of height before continuing.

### Exploring the branches ciut from the tree

The cutree() function you used in exercises 5 & 6 can also be used to cut a tree at a given height by using the h parameter. Take a moment to explore the clusters you have generated from the previous exercises based on the heights 20 & 40.

**INSTRUCTIONS**

- Build the cluster assignment vector clusters_h20 using cutree() with a h = 20
- Append the cluster assignments as a column cluster to the lineup data frame and save the results to a new dataframe called lineup_h20_complete
- Repeat the above two steps for a height of 40, generating the variables clusters_h40 and lineup_h40_complete
- Use ggplot2 to create a scatter plot, colored by the cluster assignment for both heights

```{r}
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = "complete")

# Calculate the assignment vector with a h of 20
clusters_h20 <- cutree(hc_players, h=20)

# Create a new dataframe storing these results
lineup_h20_complete <- mutate(lineup, cluster = clusters_h20)

# Calculate the assignment vector with a h of 40
clusters_h40 <- cutree(hc_players, h=40)

# Create a new dataframe storing these results
lineup_h40_complete <- mutate(lineup, cluster = clusters_h40)



# Plot the positions of the players and color them using their cluster for height = 20
ggplot(lineup_h20_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

# Plot the positions of the players and color them using their cluster for height = 40
ggplot(lineup_h40_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

### Segment wholesale customers

You're now ready to use hierarchical clustering to perform market segmentation (i.e. use consumer characteristics to group them into subgroups).

In this exercise you are provided with the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the dataframe customers_spend. Assign these clients into meaningful clusters.

Note: For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.

**INSTRUCTIONS**

- Calculate the euclidean distance between the customers and store this in dist_customers
- Run hierarchical clustering using complete linkage and store in hc_customers
- Plot the dendrogram
- Create a cluster assignment vector usign a height of 15,000 and store it as clust_customers
- Generate a new dataframe segment_customers by appending the cluster assignment as the column cluster to the orignal customers_spend dataframe

```{r warning=FALSE}
#install.packages("readx1")
#library(readxl)
customers_spend <- read_excel("customer.xlsx")
View(customers_spend)
```

```{r}
# Calculate euclidean distance between customers
dist_customers <- dist(customers_spend, method="euclidean")

# Generate a complete linkage analysis 
hc_customers <- hclust(dist_customers, method="complete")

# Plot the dendrogram
plot(hc_customers)

# Create a cluster assignment vector at h = 15000
clust_customers <- cutree(hc_customers, h=15000)

# Generate the segmented customers dataframe
segment_customers <- mutate(customers_spend, cluster = clust_customers)
```

```{r}
# Calculate euclidean distance between customers
dist_customers <- dist(customers_spend)

# Generate a complete linkage analysis 
hc_customers <- hclust(dist_customers)

# Plot the dendrogram
plot(hc_customers)

# Create a cluster assignment vector at h = 15000
clust_customers <- cutree(hc_customers, h = 15000)

# Generate the segmented customers dataframe
segment_customers <- mutate(customers_spend, cluster = clust_customers)
```

### Explore wholesale customer clusters

Continuing your work on the wholesale dataset you are now ready to analyze the characteristics of these clusters.

Since you are working with more than 2 dimensions it would be challenging to visualize a scatter plot of the clusters, instead you will rely on summary statistics to explore these clusters. In this exercise you will analyze the mean amount spent in each cluster for all three categories.

**INSTRUCTIONS**

- Calculate the size of each cluster using count().
- Color & plot the dendrogram using the height of 15,000.
- Calculate the average spending for each category within each cluster using the summarise_all() function.

```{r}
dist_customers <- dist(customers_spend)
hc_customers <- hclust(dist_customers)
clust_customers <- cutree(hc_customers, h = 15000)
segment_customers <- mutate(customers_spend, cluster = clust_customers)

# Count the number of customers that fall into each cluster
count(segment_customers, cluster)

# Color the dendrogram based on the height cutoff
dend_customers <- as.dendrogram(hc_customers)
dend_colored <- color_branches(dend_customers, h=15000)

# Plot the colored dendrogram
plot(dend_colored)

# Calculate the mean for each category
segment_customers %>% 
  group_by(cluster) %>% 
  summarise_all(funs(mean(.)))
```

# K-means clustering

### K-means on a soccer field

In the previous chapter you used the lineup dataset to learn about hierarchical clustering, in this chapter you will use the same data to learn about k-means clustering. As a reminder, the lineup dataframe contains the positions of 12 players at the start of a 6v6 soccer match.

Just like before, you know that this match has two teams on the field so you can perform a k-means analysis using k = 2 in order to determine which player belongs to which team.

Note that in the kmeans() function k is specified using the centers parameter.

**INSTRUCTIONS**

- Build a k-means model called model_km2 for the lineup data using the kmeans() function with centers = 2
- Extract the vector of cluster assignments from the model model_km2$cluster and store this in the variable clust_km2
- Append the cluster assignments as a column cluster to the lineup data frame and save the results to a new dataframe called lineup_km2
- Use ggplot to plot the positions of each player on the field and color them by their cluster

```{r}
# Build a kmeans model
model_km2 <- kmeans(lineup, centers = 2)

# Extract the cluster assignment vector from the kmeans model
clust_km2 <- model_km2$cluster

# Create a new dataframe appending the cluster assignment
lineup_km2 <- mutate(lineup, cluster = clust_km2)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_km2, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

### K-means on a soccer field (part 2)

In the previous exercise you successfully used the k-means algorithm to cluster the two teams from the lineup data frame. This time, let's explore what happens when you use a k of 3.

You will see that the algorithm will still run, but does it actually make sense in this context...

**INSTRUCTIONS**

- Build a k-means model called model_km3 for the lineup data using the kmeans() function with centers = 3
- Extract the vector of cluster assignments from the model model_km3$cluster and store this in the variable clust_km3
- Append the cluster assignments as a column cluster to the lineup data frame and save the results to a new dataframe called lineup_km3
- Use ggplot to plot the positions of each player on the field and color them by their cluster

```{r}
# Build a kmeans model
model_km3 <- kmeans(lineup, centers=3)

# Extract the cluster assignment vector from the kmeans model
clust_km3 <- model_km3$cluster

# Create a new dataframe appending the cluster assignment
lineup_km3 <- mutate(lineup, cluster = clust_km3)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_km3, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

Does this result make sense? Remember we only have 2 teams on the field. It's very important to remember that k-means will run with any k that is more than 2 and less than your total observations, but it doesn't always mean the results will be meaningful.

### Evaluating different values of K by eye -Many K's many models

While the lineup dataset clearly has a known value of k, often times the optimal number of clusters isn't known and must be estimated.

In this exercise you will leverage map_dbl() from the purrr library to run k-means using values of k ranging from 1 to 10 and extract the total within-cluster sum of squares metric from each one. This will be the first step towards visualizing the elbow plot.

**INSTRUCTIONS**

- Use map_dbl() to run kmeans() using the lineup data for k values ranging from 1 to 10 and extract the total within-cluster sum of squares value from each model: model$tot.withinss
- Store the resulting vector as tot_withinss
- Build a new dataframe elbow_df containing the values of k and the vector of total within-cluster sum of squares

```{r}
#library(purrr)

# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(lineup, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10 ,
  tot_withinss = tot_withinss
)
```

### Elbow (Scree) plot

In the previous exercises you have calculated the total within-cluster sum of squares for values of k ranging from 1 to 10. You can visualize this relationship using a line plot to create what is known as an elbow plot (or scree plot).

When looking at an elbow plot you want to see a sharp decline from one k to another followed by a more gradual decrease in slope. The last value of k before the slope of the plot levels off suggests a "good" value of k.

**INSTRUCTIONS**

Continuing your work from the previous exercise, use the values in elbow_df to plot a line plot showing the relationship between k and total within-cluster sum of squares

```{r}
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = lineup, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```

You have learned how to create and visualize elbow plots as a tool for finding a “good” value of k. In the next section you will add another tool to your aresnal for finding k.

### Silhouette analysis: Observation level performance - 

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

- Values close to 1 suggest that the observation is well matched to the assigned cluster
- Values close to 0 suggest that the observation is borderline matched between two clusters
- Values close to -1 suggest that the observations may be assigned to the wrong cluster 

In this exercise you will leverage the pam() and the silhouette() functions from the clusters library to perform silhouette analysis to compare the results of models with a k of 2 an a k of 3. You'll continue working with the lineup dataset.

Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k = 3?

**INSTRUCTIONS**

- Generate a k-means model pam_k2 using pam() with k = 2 on the lineup data.
- Plot the silhouette analysis using plot(silhouette(model)).
- Repeat the first two steps for k = 3, saving the model as pam_k3.
- Make sure to review the differences between the plots before proceeding (especially observation 3) for pam_k3.

```{r}
#library(cluster)

# Generate a k-means model using the pam() function with a k = 2
pam_k2 <- pam(lineup, k = 2)

# Plot the silhouette visual for the pam_k2 model
plot(silhouette(pam_k2))

# Generate a k-means model using the pam() function with a k = 3
pam_k3 <- pam(lineup, k = 3)

# Plot the silhouette visual for the pam_k3 model
plot(silhouette(pam_k3))
```

 Did you notice that for k = 2, no observations has a silhouette with close to 0? What about the fact that for k = 3, observation 3 is close to 0 and is negative? This suggests that k = 3 is not the right number of clusters.
 
### Revisiting wholesale data: "Best" k

At the end of Chapter 2 you explored wholesale distributor data customers_spend using hierarchical clustering. This time you will analyze this data using the k-means clustering tools covered in this chapter.
The first step will be to determine the "best" value of k using average silhouette width.

A refresher about the data: it contains records of the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the dataframe customers_spend. For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.

**INSTRUCTIONS**

- Use map_dbl() to run pam() using the customers_spend data for k values ranging from 2 to 10 and extract the average silhouette width value from each model: model silinfo avg.width Store the resulting vector as sil_width
- Build a new dataframe sil_df containing the values of k and the vector of average silhouette widths
- Use the values in sil_df to plot a line plot showing the relationship between k and average silhouette width

```{r}
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(x = customers_spend, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)
```

```{r}
sil_df[order(sil_width, decreasing=TRUE),]
```

From the plot I hope you noticed that k = 2 has the highest average sillhouette width and is the “best” value of k we will move forward with.

### Revisiting wholesale data: Exploration

From the previous analysis you have found that k = 2 has the highest average silhouette width. In this exercise you will continue to analyze the wholesale customer data by building and exploring a kmeans model with 2 clusters.

**INSTRUCTIONS**

- Build a k-means model called model_customers for the customers_spend data using the kmeans() function with centers = 2.
- Extract the vector of cluster assignments from the model model_customers$cluster and store this in the variable clust_customers.
- Append the cluster assignments as a column cluster to the customers_spend data frame and save the results to a new dataframe called segment_customers.
- Calculate the size of each cluster using count().

```{r}
# Build a k-means model for the customers_spend with a k of 2
model_customers <- kmeans(customers_spend, centers=2)

# Extract the vector of cluster assignments from the model
clust_customers <- model_customers$cluster

# Build the segment_customers dataframe
segment_customers <- mutate(customers_spend, cluster = clust_customers)

# Calculate the size of each cluster
count(segment_customers, cluster)

# Calculate the mean for each category
segment_customers %>% 
  group_by(cluster) %>% 
  summarise_all(funs(mean(.)))
```

 It seems that in this case cluster 1 consists of individuals who proportionally spend more on Frozen food while cluster 2 customers spent more on Milk and Grocery. Did you notice that when you explored this data using hierarchical clustering, the method resulted in 4 clusters while using k-means got you 2. Both of these results are valid, but which one is appropriate for this would require more subject matter expertise.
 
 