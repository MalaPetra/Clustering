---
title: "K-mean"
output: html_document
---

# Example 1: K-Means Clustering with R

K-Means is a clustering approach that belongs to the class of unsupervised statistical learning methods.

Generally, the way K-Means algorithms work is via an iterative refinement process:

- Each data point is randomly assigned to a cluster (number of clusters is given before hand).
- Each cluster’s centroid (mean within cluster) is calculated.
- Each data point is assigned to its nearest centroid (iteratively to minimise the within-cluster variation) until no    major differences are found.

Let’s have a look at an example in R using the Chatterjee-Price Attitude Data from the library(datasets) package. The dataset is a survey of clerical employees of a large financial organization. The data are aggregated from questionnaires of approximately 35 employees for each of 30 (randomly selected) departments. The numbers give the percent proportion of favourable responses to seven questions in each department.

```{r Load necessary libraries}
library(datasets)
library(readxl)
library(tidyverse)
```

```{r Inspect data structure}
str(attitude)
```

```{r summarise data}
summary(attitude)
```

As we’ve seen, this data gives the percent of favourable responses for each department. For example, in the summary output above we can see that for the variable privileges among all 30 departments the minimum percent of favourable responses was 30 and the maximum was 83. In other words, one department had only 30% of responses favourable when it came to assessing ‘privileges’ and one department had 83% of favourable responses when it came to assessing ‘privileges’, and a lot of other favourable response levels in between.

When performing clustering, some important concepts must be tackled. One of them is how to deal with data that contains multiple (or more than 2) variables. In such cases, one option would be to perform Principal Component Analysis (PCA) and then plot the first two vectors and maybe additionally apply K-Means. 

Other checks to be made are whether the data in hand should be standardized, whether the number of clusters obtained are truly representing the underlying pattern found in the data, whether there could be other clustering algorithms or parameters to be taken, etc. It is often recommended to perform clustering algorithms with different approaches and preferably test the clustering results with independent datasets. Particularly, it is very important to be careful with the way the results are reported and used.

For simplicity, we’re not going to tackle most of these concerns in this example but they should always be part of a more robust work.

In light of the example, we’ll take a subset of the attitude dataset and consider only two variables in our K-Means clustering exercise. So imagine that we would like to cluster the attitude dataset with the responses from all 30 departments when it comes to ‘privileges’ and ‘learning’ and we would like to understand whether there are commonalities among certain departments when it comes to these two variables.

```{r Subset the attitude data}
dat = attitude[,c(3,4)]; dat
```

```{r Plot subset data}
plot(dat, main = "% of favourable responses to
     Learning and Privilege", pch =20, cex =2)
```

With the data subset and the plot above we can see how each department’s score behave across Privilege and Learning compare to each other. In the most simplistic sense, we can apply K-Means clustering to this data set and try to assign each department to a specific number of clusters that are “similar”.

Let’s use the kmeans function from R base stats package:

```{r Perform K-Means with 2 clusters}
set.seed(7)
km1 = kmeans(dat, 2, nstart=100)
```

```{r k means plot results}
plot(dat, col =(km1$cluster +1) , main="K-Means result with 2 clusters", pch=20, cex=2)
```

As mentioned before, one of the key decisions to be made when performing K-Means clustering is to decide on the numbers of clusters to use. In practice, there is no easy answer and it’s important to try different ways and numbers of clusters to decide which options is the most useful, applicable or interpretable solution.

In the plot above, we randomly chose the number of clusters to be 2 for illustration purposes only.

However, one solution often used to identifiy the optimal number of clusters is called the Elbow method and it involves observing a set of possible numbers of clusters relative to how they minimise the within-cluster sum of squares. In other words, the Elbow method examines the within-cluster dissimilarity as a function of the number of clusters. Below is a visual representation of the method:

```{r Check for the optimal number of clusters given the data}
mydata <- dat
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata,
                                       centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",
     main="Assessing the Optimal Number of Clusters with the Elbow Method",
     pch=20, cex=2)
```

With the Elbow method, the solution criterion value (within groups sum of squares) will tend to decrease substantially with each successive increase in the number of clusters. Simplistically, an optimal number of clusters is identified once a “kink” in the line plot is observed. As you can grasp, identifying the point in which a “kink” exists is not a very objective approach and is very prone to heuristic processes.

But from the example above, we can say that after 6 clusters the observed difference in the within-cluster dissimilarity is not substantial. Consequently, we can say with some reasonable confidence that the optimal number of clusters to be used is 6.

Assuming this assertion is valid, we can go on and apply the identified number of clusters onto the K-Means algorithm and plot the results:

```{r Perform K-Means with the optimal number of clusters identified from the Elbow method}
set.seed(7)
km2 = kmeans(dat, 6, nstart=100)
```

```{r Examine the result of the clustering algorithm}
km2
```

```{r Plot results}
plot(dat, col =(km2$cluster +1) , main="K-Means result with 6 clusters", pch=20, cex=2)
```

From the results above we can see that there is a relatively well defined set of groups of departments that are relatively distinct when it comes to answering favourably around Privileges and Learning in the survey. It is only natural to think the next steps from this sort of output. One could start to devise strategies to understand why certain departments rate these two different measures the way they do and what to do about it. But we will leave this to another exercise.

In this example we looked at the concept of K-Means clustering and showed a very brief example of its application highlighting the results and the potential concerns that arise from such approaches.

*Author: Felipe Rego, July 2015*

# Example 2: Introduction to the Pokemon data

The first challenge with the Pokemon data is that there is no pre-determined number of clusters. You will determine the appropriate number of clusters, keeping in mind that in real data the elbow in the scree plot might be less of a sharp elbow than in synthetic data. Use your judgement on making the determination of the number of clusters.

The second part of this exercise includes plotting the outcomes of the clustering on two dimensions, or features, of the data. These features were chosen somewhat arbitrarily for this exercise. Think about how you would use plotting and clustering to communicate interesting groups of Pokemon to other people.

An additional note: this exercise utilizes the iter.max argument to kmeans(). As you’ve seen, kmeans() is an iterative algorithm, repeating over and over until some stopping criterion is reached. The default number of iterations for kmeans() is 10, which is not enough for the algorithm to converge and reach its stopping criterion, so we’ll set the number of iterations to 50 to overcome this issue. To see what happens when kmeans() does not converge, try running the example with a lower number of iterations (e.g. 3). This is another example of what might happen when you encounter real data and use real cases.

Instructions

The pokemon dataset, which contains observations of 800 Pokemon characters on 6 dimensions (i.e. features), is available in your workspace.

- Using kmeans() with nstart = 20, determine the total within sum of square errors for different numbers of clusters (between 1 and 15).
- Pick an appropriate number of clusters based on these results from the first instruction and assign that number to k.
- Create a k-means model using k clusters and assign it to the km.out variable.
- Create a scatter plot of Defense vs. Speed, showing cluster membership for each observation.

```{r Load Data}
#library(readxl)

pokemon <- read_excel("pokemon.xlsx", sheet = 1)
```

```{r filtering pokemon data}
#library(tidyverse)
x <- pokemon%>%select(Attack, Defense, SpecialAtk, SpecialDef, Speed)

##   HitPoints Attack Defense SpecialAttack SpecialDefense Speed
## 1        45     49      49            65             65    45
## 2        60     62      63            80             80    60
## 3        80     82      83           100            100    80
## 4        80    100     123           122            120    80
## 5        39     52      43            60             50    65
## 6        58     64      58            80             65    80
```

```{r}
#Set up 1x1 plotting grid
par(mfrow = c(1, 1))

# Initialize total within sum of squares error: wss
wss <- 0
```

```{r Look over 1 to 15 possible clusters}
for (i in 1:15) {
  # Fit the model: km.out
  km.out <- kmeans(x, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.out$tot.withinss
}
```

```{r Produce a scree plot}
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")
```

```{r Select number of clusters}
k <- 3
```

``` {r Build model with k clusters: km.out}
km.out <- kmeans(x, centers = k, nstart = 50, iter.max = 50)
# I have to use x object which does not contain discrete variables
```

```{r View the resulting model}
km.out
```

```{r plot defense vs. speed by cluster membership}
plot(x[, c("Defense", "Speed")],
     col = km.out$cluster,
     main = paste("k-means clustering of Pokemon with", k, "clusters"),
     xlab = "Defense", ylab = "Speed")
```

# Example 3: k-Means Clustering on Iris data

```{r loading iris dataset}
data("iris") #load Iris Dataset
str(iris) # view structure of dataset
```

```{r summary of iris}
summary(iris) #view statistical summary of dataset
```

```{r top row of iris}
head(iris)
```

Since clustering is a type of Unsupervised Learning, we would not require Class Label(output) during execution of our algorithm. We will, therefore, remove Class Attribute “Species” and store it in another variable. We would then normalize the attributes between 0 and 1 using our own function.

```{r removing attributes}
iris.new<- iris[,c(1,2,3,4)]
iris.class<- iris[,"Species"]

head(iris.new)
head(iris.class)
```

```{r normalisation}
normalize <- function(x){
  return ((x-min(x))/(max(x)-min(x)))
}

iris.new$Sepal.Length<- normalize(iris.new$Sepal.Length)
iris.new$Sepal.Width<- normalize(iris.new$Sepal.Width)
iris.new$Petal.Length<- normalize(iris.new$Petal.Length)
iris.new$Petal.Width<- normalize(iris.new$Petal.Width)
head(iris.new)
```

```{r apply k-means clustering algorithm}
result<- kmeans(iris.new,3) #aplly k-means algorithm with no. of centroids(k)=3
result$size # gives no. of records in each cluster

result$centers # gives value of cluster center datapoint value(3 centers for k=3)

result$cluster #gives cluster vector showing the custer where each record falls
```

```{r verifing results of clustering}
# Plot to see how Sepal.Length and Sepal.Width data points have been distributed in clusters

par(mfrow=c(2,2), mar=c(5,4,2,2))
plot(iris.new[c(1,2)], col=result$cluster)
```

```{r}
# Plot to see how Sepal.Length and Sepal.Width data points have been distributed originally as per "class" attribute in dataset
plot(iris.new[c(1,2)], col=iris.class)
```

```{r}
# Plot to see how Petal.Length and Petal.Width data points have been distributed in clusters
plot(iris.new[c(3,4)], col=result$cluster)
```

```{r}
plot(iris.new[c(3,4)], col=iris.class)
```

```{r}
# Table with clusters
table(result$cluster, iris.class)
```

Result of table shows that Cluster 1 corresponds to Virginica, Cluster 2 corresponds to Versicolor and Cluster 3 to Setosa.

Total number of correctly classified instances are: 36 + 47 + 50= 133

Total number of incorrectly classified instances are: 3 + 14= 17

Accuracy = 133/(133+17) = 0.88 i.e our model has achieved 88% accuracy!

